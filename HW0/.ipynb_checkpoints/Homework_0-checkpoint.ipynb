{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46880751-3d32-42ef-a196-a051b171e751",
   "metadata": {},
   "source": [
    "### (0%) Familiarize yourself with Github and basic git\n",
    "Github (https://github.com) is the de facto platform for open source software in the world based on the very popular git (https://git-scm.org) version control system. Git has a sophisticated set of tools for version control based on the concept of local repositories for fast commits and remote repositories only when collaboration and remote synchronization is necessary. Github enhances git by providing tools and online hosting of public and private repositories to encourage and promote sharing and collaboration. Github hosts some of the world’s most widely used open source software.\n",
    "If you are already familiar with git and Github, then this part will be very easy!\n",
    "### § Task: Create a public Github repo named \"mcis6273-f23-datamining\" and place a readme.md file in it. Create your first file called README.md at the top level of the repository.\n",
    "Please put your Zotero username in the file. Aside from that you can put whatever text you like in the file (If you like, use something like lorem ipsum to generate random sentences to place in the file.). Please include the link to your Github repository that now includes the minimal README.md. You don’t have to have anything elaborate in that file or the repo.\n",
    "### § Task: Fork the course repository:\n",
    "https://github.com/kmsaumcis/mcis6273_f23_datamining/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78ef6be5-c851-4f3c-818d-f1cae335225b",
   "metadata": {},
   "source": [
    "https://github.com/PavaniPothu/mcis6273-f23-datamining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42015ce-9d9c-4311-8a9c-8a2c0895ea15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8df3aa9-9426-44e3-ac55-f2add84d2eca",
   "metadata": {},
   "source": [
    "### (10%) Familiarize yourself with the JupyterLab environment, Markdown and Python\n",
    "As stated in the course announcement Jupyter (https://jupyter.org) is the core platform we will be using in this course and is a popular platform for data scientists around the world. We have a JupyterLab setup for this course so that we can operate in a cloud-hosted environment, free from some of the resource constraints of running Jupyter on your local machine (though you are free to set it up on your own and seek my advice if you desire).\n",
    "You have been given the information about the Jupyter environment we have setup for our course, and the underlying Python environment will be using is the Anaconda (https://anaconda.com) distribution. It is not necessary for this assignment, but you are free to look at the multitude of packages installed with Anaconda, though we will not use the majority of them explicitly.\n",
    "As you will soon find out, Notebooks are an incredibly effective way to mix code with narrative and you can create cells that are entirely code or entirely Markdown. Markdown (MD or md) is a highly readable text format that allows for easy documentation of text files, while allowing for HTML-based rendering of the text in a way that is style-independent.\n",
    "We will be using Markdown frequently in this course, and you will learn that there are many different “flavors” or Markdown. We will only be using the basic flavor, but you will benefit from exploring the “Github flavored” Markdown, though you will not be responsible for using it in this course – only the “basic” flavor. Please refer to the original course announcement about Markdown.\n",
    "\n",
    "### § Task: THERE IS NOTHING TO TURN IN FOR THIS PART.\n",
    "Play with and become familiar with the basic functions of the Lab environment given to you online in the course blackboard.\n",
    "\n",
    "### § Task: Please create a markdown document called semester_goals.md with 3 sentences/fragments that answer the following question:\n",
    "What do you wish to accomplish this semester in Data Mining?\n",
    "Read the documentation for basic Markdown here. Turn in the text .md file not the processed .html. In whatever you turn in, you must show the use of ALL the following:\n",
    "* headings (one level is fine),\n",
    "* bullets,\n",
    "* bold and italics\n",
    "\n",
    "Again, the content of your document needs to address the question above and it should live in the top level directory of your assignment submission. This part will be graded but no points are awarded for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f55d3-c242-4dbd-bda1-f022fa5b98dd",
   "metadata": {},
   "source": [
    "#### -------> The markdown file named \"semester_goals.md\" is submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7dde50-2996-4c89-a624-acc488ea1bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "431b1240-9310-4260-ad86-271eb3b6d5f7",
   "metadata": {},
   "source": [
    "### (0%) Explore JupyterHub Linux console integrating what you learned in the prior parts of this homework\n",
    "The Linux console in JupyterLab is a great way to perform command-line tasks and is an essential tool for basic scripting that is part of a data scientist’s toolkit. Open a console in the lab environment and familiarize yourself with your files and basic commands using git as indicated below.\n",
    "In a new JupyterLab command line console, run the git clone command to clone the new repository you created in the prior part. You will want to read the documentation on this command (try here https://www.git-scm.com/docs/git-clone to get a good start).\n",
    "Within the same console, modify your README.md file, check it in and push it back to your repository, using git push. Read the documentation about git push.\n",
    "The commands wget and curl are useful for grabbing data and files from remote resources off the web. Read the documentation on each of these commands by typing man wget or man curl in the terminal. Make sure you pipe the output to a file or use the proper flags to do so.\n",
    "\n",
    "### § Task: THERE IS NOTHING TO TURN IN FOR THIS PART."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600d76b-ed7e-48ac-8579-d67273e25759",
   "metadata": {},
   "source": [
    "#### ------->  As instructed the above operations are performed in jupyter console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af0324-7cc8-4f7c-9a2a-d0424dc349b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d9ef010-dc40-47e2-bfa8-0240967ffbf0",
   "metadata": {},
   "source": [
    "### (30%) Listen to the Talk Python To Me from July 7, 2023: How data scientists use Python\n",
    "Data science is one of the most important and “hot” disciplines today and there is a lot going on from data engineering to modeling and analysis. Python is critial to the data scientists toolkit, but they are interesting in their own right.\n",
    "\n",
    "Why?\n",
    "\n",
    "In this short, interesting and informative podcast, you will learn about the reasons why Python is so hot, and how Python made it to the top of the data science stack.\n",
    "\n",
    "Please listen to this one hour podcast and answer some of the questions below. You can listen to it from one of the two links below:\n",
    "* Talk Python[‘Podcast’] https://talkpython.fm/episodes/show/422/how-data-scientists-use-python\n",
    "* direct link to mp3 file https://talkpython.fm/episodes/download/422/how-data-scientists-use-python.mp3\n",
    "\n",
    "### § Task: PLEASE ANSWER THE FOLLOWING QUESTIONS AFTER LISTENING TO THE PODCAST:\n",
    "* List 3 things that you learned from this podcast?\n",
    "* What is your reaction to the podcast? Pick at least one point brought up in the interview that you agree with and list your reason why.\n",
    "* After listening to the podcast, do you think you are more informed about the importance of Python to Data Science? How? (Be brief – one sentence will suffice.)\n",
    "* List one surprising fact you learned from listening to this podcast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036b1fc-91d6-4c2e-9735-6c7be4b44048",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------------------------\n",
    "##### Answers for podcast:\n",
    "1. After listening to the podcast, the three things I have learnt are:\n",
    "    * Python language plays a key role in performance improvements as it has many libraries and tools required for the field of data science and by adopting them we can derive better solutions.\n",
    "    * As said earlier it has a good set of libraries and one of them is Pandas. I have learnt that pandas play a crucial role while doing data analysis at any stage either data cleaning or working with data frames and manipulation of data.\n",
    "    * Also learnt that JupyterLite uses Pyodide and Pyscript, which are CPython running a WebAssembly. \n",
    "\n",
    "2. After listening to the podcast I was surprised that there are many things that I'm still not aware of regarding data analysis or required for data science. I agree with the point that staying curious and open to trying out new libraries and tools in the field of data science. Being willing to explore new technologies can lead to better solutions and improved efficiency.\n",
    "\n",
    "3. Yes, after listening to the podcast I believe that I was informed or learnt the importance of Python to Data Science. Python has many libraries which lead to ease of managing huge datasets using libraries like Pandas, and Numpy.\n",
    "\n",
    "4. There are many surprising facts in the entire podcast and one of them is the discussion regarding XKCD-Graphs because I have never heard of them and when I explored the internet found that the speakers were true about XKCD-Graphs\n",
    "\n",
    "### ---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf1148-f70d-4abf-b8a7-9e0c5817e986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e620526a-8c44-4ca3-a53a-ce33d796f2bb",
   "metadata": {},
   "source": [
    "### (30%) Perfom basic data engineering in Python using Gutenberg.org text of Bertrand Russell’s 1912 work The Problems of Philosophy\n",
    "You learned from the prior part that data science is one of Python’s strengths.\n",
    "\n",
    "In this part, you will interact directly with those strengths, but in a way that will allow you to see the challenges that you will face and confront as a real-world data scientist.\n",
    "\n",
    "Data engineering as you have learned from the readings is about transforming data from one form to another so that it can be used in the appropriate analysis contexts.\n",
    "\n",
    "One area of intense work is in transforming unstructured data, like a book or text, into structured data. More importantly, producing statistical analyses of these unstructured data is often difficult, because one must convert that unstructured data to something that a machine can process algorithmically.\n",
    "\n",
    "In this part of the homework you will take a text from the Project Gutenberg https://gutenberg.org and convert it to something more structured. In fact, you will convert it to multiple structured forms.\n",
    "\n",
    "For this part we will be working with Betrand Russell’s 1912 work The Problems of Philosphy which is located at the Project Gutenberg’s website (https://gutenberg.org/)!. The .txt file you will want to work with is here:\n",
    "https://www.gutenberg.org/cache/epub/5827/pg5827.txt\n",
    "If you are not familiar with Betrand Russell, you may want to be. He is widely regarded as an important and influential 20th century western logician, mathematician and philosopher who made prolific, deep and crucial contributions to the philosophy of mathematics, logic, set theory, computer science, artificial intelligence, epistemology and metaphysics.\n",
    "Additionally, if you are unfamiliar with Project Gutenberg, you can learn more about it here: https://gutenberg.org/about/background/. It is an essential repository of many classic books and texts which are now out of copyright, but more importantly it’s founder, Michael Hart, invented eBooks in 1971, before probably all of us were born, and certainly before the widespread ubiquity of the public Internet as we know it. It is a fascinating history that you should know a little about.\n",
    "\n",
    "For our purposes, though, what makes Gutenberg most interesting is that we can directly obtain the .txt version of the texts allowing us to use the power of Python to computationally process this unstructured data and convert it to something more useful to our machines and algorithms.\n",
    "\n",
    "Your code must be implemented in Jupyter as a notebook – you will be required to turn in a .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f0f18-c4a7-4153-8832-7de3cddefc94",
   "metadata": {},
   "source": [
    "### § Task: Use Python to parse and tokenize the text file.\n",
    "You will produce a .csv file which will have all the full words lowercase and with all punctuation removed unless it is part of the word. For example, if you have a token “world.”, you will drop the ending period, however, if you have a word “can't”, you will retain the apostrophe “'”.\n",
    "Your output .csv file will contain all the words in alphabetical order with their frequency counts.\n",
    "Here is an example of some lines in such a .csv file:<br>\n",
    "... <br>\n",
    "<br>\n",
    "the,112 <br>\n",
    "there,62 <br>\n",
    "thing,3 <br>\n",
    "this,200 <br>\n",
    "<br>\n",
    "... <br>\n",
    "\n",
    "NOTE: Only the words (first column) are sorted, the counts do not need to be sorted.\n",
    "Please name your file all_words.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ddcafd-df36-4b37-874f-1f019219c98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the Lowercase words...\n",
      "Filtering of Lowercase words is done...Now moving to push the output into a csv file\n",
      "A csv file named all_words.csv has been successfully created\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import string\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/5827/pg5827.txt\"\n",
    "response = requests.get(url) #reading the txt from the given url as response\n",
    "text = response.text #changing the response to text format for further progress\n",
    "\n",
    "print(\"Filtering the Lowercase words...\")\n",
    "lowercase_words = text.split()\n",
    "lowercase_words = [word for word in lowercase_words if word[0].islower()] #getting lowercase words from the text file\n",
    "lowercase_words = [word.strip(string.punctuation) for word in lowercase_words] #stripping out the punctuations as instructed\n",
    "lowercase_words = [word for word in lowercase_words if not any(char.isdigit() for char in word)] #checking for digits within the words\n",
    "word_count = Counter(lowercase_words)\n",
    "sorted_word_counts = sorted(word_count.items(), key=lambda x: x[1], reverse=True) #using lambda function to sort with respect to count for later purpose\n",
    "\n",
    "print(\"Filtering of Lowercase words is done...Now moving to push the output into a csv file\")\n",
    "with open(\"all_words.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile: #writing output to csv file as instructed\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow([\"Word\", \"Frequency\"])\n",
    "    for word, frequency in sorted_word_counts:\n",
    "        csv_writer.writerow([word, frequency])\n",
    "\n",
    "print(\"A csv file named all_words.csv has been successfully created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a00ee-aaec-4220-803f-d5d8bc91fddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4f7fa5c-ffe9-47f0-9e6b-9d6ba746ff38",
   "metadata": {},
   "source": [
    "### § Task: Now that we have all the words, let’s go back to the drawing board and get all capitalized (uppercase) words.\n",
    "To do this, you will tokenize as before, but you will retain only those words that are capitalized.\n",
    "\n",
    "Also, as before, you will remove punctuation except when it is part of the word, such as an example of a possessive proper noun like “Carl's”.\n",
    "\n",
    "You will also include the frequency counts of these capitalized words in sorted order by word.\n",
    "\n",
    "Please name your file all_uppercase_words.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0154b3dc-b6df-4f95-9235-96896a9716ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the Uppercase words...\n",
      "Filtering of Uppercase words is done...Now moving to push the output into a csv file\n",
      "A CSV file named 'all_uppercase_words.csv' has been successfully created.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import string\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/5827/pg5827.txt\"\n",
    "response = requests.get(url)#reading the txt from the given url as response\n",
    "text = response.text #changing the response to text format for further progress\n",
    "\n",
    "print(\"Filtering the Uppercase words...\")\n",
    "uppercase_words = text.split()\n",
    "uppercase_words = [word for word in uppercase_words if word[0].isupper()]  #getting uppercase words from the text file\n",
    "uppercase_words = [word.strip(string.punctuation) for word in uppercase_words]  #stripping out the punctuations as instructed\n",
    "uppercase_words = [word for word in uppercase_words if not any(char.isdigit() for char in word)]  #checking for digits within the words\n",
    "word_count = Counter(uppercase_words) \n",
    "sorted_word_counts = sorted(word_count.items(), key=lambda x: x[1], reverse=True) #using lambda function to sort with respect to count for later purpose\n",
    "\n",
    "print(\"Filtering of Uppercase words is done...Now moving to push the output into a csv file\")\n",
    "with open(\"all_uppercase_words.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile: #writing output to csv file as instructed\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow([\"Word\", \"Frequency\"])\n",
    "    for word, frequency in sorted_word_counts:\n",
    "        csv_writer.writerow([word, frequency])\n",
    "\n",
    "print(\"A CSV file named 'all_uppercase_words.csv' has been successfully created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd7f49-f24e-460a-a0c1-cc270209b922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67b2fae6-faf6-4519-9937-7a6d0a5bbaac",
   "metadata": {},
   "source": [
    "### § Task: Answer the following questions:\n",
    "\n",
    "1. Which were the 5 most frequent words in all_words.csv were most frequent?\n",
    "\n",
    "2. Which were the 5 most frequent words in all_uppercase_words.csv.\n",
    "\n",
    "3. Compare and contrast these top 5. Explain in 2-3 sentences what you observe about the similariries and differences.\n",
    "\n",
    "4. In your own words, what were the most surprising parts of each list?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9c01e-c9ab-44f1-8fa4-14909430d8c1",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------------------------\n",
    "1. In the file \"all_words.csv\" 5 most frequent words and their is as follows, <br>\n",
    "\n",
    "|Word      | Frequency|\n",
    "|--------- | ---------|\n",
    "|the       |2498      |\n",
    "|of        |1859      |\n",
    "|is        |1314      |\n",
    "|to        |1231      |\n",
    "|and       |981       |\n",
    "\n",
    "\n",
    "2. In the file \"all_uppercase_words.csv\" 5 most frequent words are as follows, <be>\n",
    "\n",
    "|Word      | Frequency|\n",
    "|--------- | ---------|\n",
    "|The       |187       |\n",
    "|I         |137       |\n",
    "|But       |119       |\n",
    "|It        |117       |\n",
    "|Thus      |113       |\n",
    "\n",
    "\n",
    "3. According to my analysis, all the top 5 words in both the CSV files were mostly conjunctions and connectives. In both the sets, the top 1st word was \"the\"/\"The\" with the highest count as it is most used while writing anything.\n",
    "\n",
    "4. In my opinion, the most surprising part of each list was the word \"the\"/\"The\" topping the count. And we all know that uppercase or capital letters are mostly used while starting a sentence so uppercase word count might be low.\n",
    "\n",
    "### ---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335b72d-8e36-4e8f-b6d6-f2c099c7be48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "986ae49d-0a38-4c31-9dfb-956626968a55",
   "metadata": {},
   "source": [
    "### (30%) Use structured data to develop basic statistical analyses\n",
    "\n",
    "Now that we have a sense of taking this text and producing some output files that are quite a bit more interesting, we are going to go further into some statistical analyses.\n",
    "\n",
    "Of course, one thing that we are concerned about in unstructured data, are elements that do not add much to our understanding or conversion of that data.\n",
    "\n",
    "One such area in the English language, at least (and most other languages), are words that do not increase the information of the sentence at an essential level.\n",
    "\n",
    "For example, the word 'the' is not a very useful word when analyzing text, and especially the words that add to the meaning of a sentence. It is usually the nouns and verbs that get us to the useful parts, and then the pronouns, adjectives, adverbs, etc. Critically, the less common a word is, the more likely that word is important to understanding a text.\n",
    "\n",
    "We are going to delve into a basic and rudimentary statistical analysis of the text.\n",
    "\n",
    "When we are done, we should be able to answer a question like How likely is it to see a sentence with the words car, plant, simple? We will also continue some basic data engineering along the way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1617a-a2c9-4af5-8008-ec7ca9c72a20",
   "metadata": {},
   "source": [
    "### § Task: Remove the stopwords from your all_words.csv and put the remaining non-stopwords in a file all_ns_words.csv. Please retain the frequency column as before.\n",
    "\n",
    "A good list of stopwords to start with can be found here:\n",
    "* https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt\n",
    "  \n",
    "Furthermore, you can learn what a stopword is from the excellent text Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008. https://nlp.stanford.edu/IR-book/. :\n",
    "* here is primary source information on stopwords https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd4c87f6-344d-45c3-8c97-88504ac5ac10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the previous CSV file 'all_words.csv'...\n",
      "Filtering the stop words...writing into new CSV file as instructed with name 'all_ns_words.csv'\n",
      "CSV file 'all_ns_words.csv' has been created with non-stopwords and thier frequencies.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt\" #as we did before reading all the words as instrcuted above using requests.get method\n",
    "response = requests.get(url)\n",
    "stopwords = set(response.text.split())\n",
    "\n",
    "print(\"Loading the previous CSV file 'all_words.csv'...\")\n",
    "with open(\"all_words.csv\", \"r\", encoding=\"utf-8\") as csvfile: #as instructed we are loading the CSV file that we already created i.e., \"all_words.csv\"\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    header = next(csv_reader)\n",
    "    wf = list(csv_reader)\n",
    "\n",
    "print(\"Filtering the stop words...writing into new CSV file as instructed with name 'all_ns_words.csv'\")\n",
    "ns = [(word, freq) for word, freq in wf if word.lower() not in stopwords] #As we did earlier filtering the stopwords\n",
    "\n",
    "with open(\"all_ns_words.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile: #Now writing the output to new file with non-stop words\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow([\"Word\", \"Frequency\"])\n",
    "    csv_writer.writerows(ns)\n",
    "\n",
    "print(\"CSV file 'all_ns_words.csv' has been created with non-stopwords and thier frequencies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e371fe-d154-4c8e-bd26-556843c0aaca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd3d08cf-c78e-4fcc-b97d-ed41a973bae7",
   "metadata": {},
   "source": [
    "### § Task: Add a new column to your all_ns_words.csv that contains the probability of that word.\n",
    "To do this, use the denomator of the sum of stopwords not all words. Alternatively, do not include stopword counts in your sum.\n",
    "\n",
    "Thus, W are all words and if w is a non-stopword, w ∈ W, let Cw be the frequency (count) of word w. Thus, <br> \n",
    "$\\Pr(w\\in W) = \\frac{C_w}{\\sum_{w'\\in W}C_{w'}}$\n",
    "\n",
    "Concretely, if “righteous” appears 200 times, and the sum of frequencies of all non-stopwords is 10000, then $\\Pr(w=righteous) = \\frac{200}{10000} = 0.02$.\n",
    "Your new file will look something like:\n",
    "\n",
    "... <br> \n",
    "friend, 112, .003 <br>\n",
    "fruit, 67, .00014 <br>\n",
    "grand, 88, .01763 <br>\n",
    "... <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30bf9399-63f9-4d24-9810-83520316af68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the previous CSV file 'all_ns_words.csv'...\n",
      "Writing the updated CSV file 'all_ns_words.csv' with probabilities...\n",
      "CSV file 'all_ns_words.csv' has been updated with probabilities.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the previous CSV file 'all_ns_words.csv'...\")\n",
    "with open(\"all_ns_words.csv\", \"r\", encoding=\"utf-8\") as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    header = next(csv_reader)\n",
    "    wf = list(csv_reader)\n",
    "\n",
    "sum_ns = sum(int(freq) for word, freq in wf) #Calculating the sum of frequencies of non-stopwords\n",
    "prob = []\n",
    "for word, freq in wf: #Calculating the probablility\n",
    "    probability = int(freq) / sum_ns\n",
    "    prob.append((word, freq, f\"{probability:.5f}\")) #Appending into the list\n",
    "\n",
    "print(\"Writing the updated CSV file 'all_ns_words.csv' with probabilities...\")\n",
    "with open(\"all_ns_words.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow([\"Word\", \"Frequency\", \"Probability\"])\n",
    "    csv_writer.writerows(prob) #adding the calculated probability list into the csv\n",
    "\n",
    "print(\"CSV file 'all_ns_words.csv' has been updated with probabilities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b7122-c814-4124-a473-5af236410d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd58a1c8-3867-4490-90bf-0baeff3cab80",
   "metadata": {},
   "source": [
    "### § Task: Answer the following questions using your analysis and results from the text:\n",
    "1. How many unique non-stop words are in the text?\n",
    "2. Which is the least probable word? (if there is a tie, please state the tie words)\n",
    "3. What observation can you make about the probabilities?\n",
    "4. Which sentence is more likely:\n",
    "    1. If a belief is true, it can be deduced it is universal.\n",
    "    2. Criticism of knowledge is counter to scientific results.\n",
    "\n",
    "    You will use the sum of the probabilities of each non-stop word to answer the question. You will need to give numeric rationale for your answer. Show your work in Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38b667-9f88-4f98-b322-751361207009",
   "metadata": {},
   "source": [
    "#### -----------------------------------------------------------------------------------------------------------------------\n",
    "1. How many unique non-stop words are in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65cc71fb-a43a-4a21-929a-d477466f63d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique non-stop words in the text are:  2738\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique non-stop words in the text are: \", len(prob)) #here prob is the previous list we have and we are finding its length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f635099-effb-4bbe-b5fd-19cd570f5e1a",
   "metadata": {},
   "source": [
    "2. Which is the least probable word? (if there is a tie, please state the tie words)\n",
    "    * When I have explored the CSV file \"all_ns_words.csv\", I have understood that there are many unique words with same probability value so limiting the output to only 25 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291995a7-016b-410d-80cb-5906df919026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Least probable words are:\n",
      " [('knowledge', '288', '0.02376'), ('true', '129', '0.01064'), ('sense-data', '114', '0.00941'), ('object', '102', '0.00842'), ('belief', '100', '0.00825'), ('physical', '96', '0.00792'), ('objects', '94', '0.00776'), ('relation', '92', '0.00759'), ('sense', '89', '0.00734'), ('table', '87', '0.00718'), ('truth', '87', '0.00718'), ('mind', '83', '0.00685'), ('experience', '76', '0.00627'), ('acquaintance', '75', '0.00619'), ('space', '72', '0.00594'), ('philosophy', '70', '0.00578'), ('acquainted', '70', '0.00578'), ('proposition', '61', '0.00503'), ('time', '60', '0.00495'), ('question', '59', '0.00487'), ('colour', '56', '0.00462'), ('universals', '56', '0.00462'), ('priori', '55', '0.00454'), ('exist', '54', '0.00446'), ('principle', '54', '0.00446')]\n"
     ]
    }
   ],
   "source": [
    "print(\"25 Least probable words are:\\n\", prob[:25]) #As there is tie in the least probable words we are limiting the output to 25 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cdc815-3299-41f8-883d-741edaf9a330",
   "metadata": {},
   "source": [
    "3. What observation can you make about the probabilities?\n",
    "    * As a part of observations we are now performing statiscal analysis on the probabilites derived, calculating \"*Mean*\" and \"*Standard Deviation*\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d32a2e0-af7d-4576-9f71-31a164c95af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Probability for the words is: 0.0003652629656683567\n",
      "Standard Deviation for the probability of the words is: 0.0008900710037797126\n"
     ]
    }
   ],
   "source": [
    "n_prob = [float(p) for _, _, p in prob]\n",
    "mean = sum(n_prob) / len(n_prob) #Calculating Mean\n",
    "print(\"Mean Probability for the words is:\", mean)\n",
    "stnd_deviation = (sum((x - mean) ** 2 for x in n_prob) / len(n_prob)) ** 0.5 #Calculating Standard Deviation\n",
    "print(\"Standard Deviation for the probability of the words is:\", stnd_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b4d54-40e5-448c-a207-a6ef0cf0066d",
   "metadata": {},
   "source": [
    "4. Which sentence is more likely: <br>\n",
    "    A. If a belief is true, it can be deduced it is universal.<br>\n",
    "    B. Criticism of knowledge is counter to scientific results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b51b56f-4e1a-4290-b5ca-fbfb04d31698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With respect to our previous analysis, Sentence B is more likely.\n"
     ]
    }
   ],
   "source": [
    "senA = \"If a belief is true, it can be deduced it is universal.\"\n",
    "senB = \"Criticism of knowledge is counter to scientific results.\"\n",
    "\n",
    "def likely_sen(sentence, prob):\n",
    "    words = sentence.split()\n",
    "    prob_sum = sum(float(probability) for word, _, probability in prob if word in words)\n",
    "    return prob_sum\n",
    "\n",
    "if likely_sen(senA, prob) > likely_sen(senB, prob): #Comparing sum of probability of words of sentence A against B\n",
    "    print(\"With respect to our previous analysis, Sentence A is more likely.\")\n",
    "elif likely_sen(senB, prob) > likely_sen(senA, prob): #Comparing sum of probability of words of sentence B against A\n",
    "    print(\"With respect to our previous analysis, Sentence B is more likely.\")\n",
    "else:\n",
    "    print(\"With respect to our previous analysis, Both sentences have the same likelihood.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe81df-36c0-4ef8-888c-3f838e54f472",
   "metadata": {},
   "source": [
    "#### -----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a949f9dc-7315-4d95-8a14-7575b04f2cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a122d44-8a23-4fef-8379-e3eec8cacd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
